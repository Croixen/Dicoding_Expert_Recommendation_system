# -*- coding: utf-8 -*-
"""Rev.2 Submission_Akhir_Dicoding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cks5_eoILKDTgBrMxJIqrAyvNDfP1v8z

#AKUSISI DATASET
"""

!pip install kaggle

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download "nicoletacilibiu/movies-and-ratings-for-recommendation-system"

!unzip movies-and-ratings-for-recommendation-system.zip

"""# DATA LOADING"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import numpy as np

from tensorflow.keras import layers, ops
from tensorflow import keras
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors

df_movies = pd.read_csv("movies.csv")
df_ratings = pd.read_csv("ratings.csv")

"""# EDA"""

df_movies.info()

df_ratings.info()

df_ratings['rating'].describe()

# Menggabungkan data rating dan film menjadi satu.
df = df_movies.merge(df_ratings, on = 'movieId', how = "left")
df

df.duplicated().any()
#Melihat apakah terdapat data yang terduplikasi atau tidak

df.isna().sum()
#Untuk melihat apakah terdapat data dengan salah satu kolom atau lebih yang
#merupakan data non angka

rating_group = df.groupby(['title', 'rating']).size().reset_index(name='count')
rating_group = rating_group.sort_values(by='count', ascending=False)

avg_rating = df.groupby('title')['rating'].mean().reset_index(name='avg_rating')

rating_group = rating_group.merge(avg_rating, on='title')
rating_group.head(10)

#Melakukan grouping berdasar pada judul dan rating,
#Grouping ini akan diberikan agregasi untuk mencari jumlah review yang diberikan.
# Kemudian akan diberikan rata rata dari rating yang diberikan oleh user.

df.isna().sum()

"""### Visualisasi"""

plt.figure(figsize=(10,6))
sns.histplot(rating_group['rating'], bins=5, kde=True)
plt.title('Distribusi Rata Rata Rating per Title')
plt.xlabel('Average Rating')
plt.show()

df_genres = df['genres'].str.split('|').explode()

genre_counts = df_genres.value_counts()

plt.figure(figsize=(12,8))
sns.barplot(x=genre_counts.values, y=genre_counts.index)
plt.title('Distribusi Genre')
plt.xlabel('Count')
plt.ylabel('Genre')
plt.show()

# Ekstrak tahun
df['year'] = df['title'].str.extract(r'\((\d{4})\)').astype(float)
plt.figure(figsize=(12,8))

# Hilangkan tahun yang hilang
years = df['year'].dropna()

# Plot histogram dengan KDE
sns.histplot(years, bins=10, kde=True, fill = False)

plt.title('Film per Tahun')
plt.xlabel('Year')
plt.ylabel('Density')
plt.show()

"""## Insight
Dalam graph yang telah di buat, terdapat beberapa insight yang didapatkan.

- Dataset ini memiliki film yang dimulai sekitar tahun 1900 awal hingga pada sebelum tahun 2020 (sekitar tahun 2018).
- Dari dataset tersebut, dapat dilihat bahwa film yang paling banyak direview oleh pengguna adalah film dari tahun sekitar 1990 hingga pada tahun 2020, dengan puncaknya ada disekitar tahun 200-an
- Genre yang paling sering ditonton oleh pengguna adalah genre Drama. Diikuti dengan Komedi, Action, Adventure, Romance, Scifi, dan Crime. Kita mungkin akan sering mendapatkan rekomendasi yang berkisar pada genre ini.
- User cenderung memberikan rating sekitar 2.5 hingga 4.3.

# Preprocessing

## Missing Value dan Duplicated

Di tahap ini, jika memang ada nilai yang kosong dan juga nilai duplikat, maka akan dilakukan sebuah treatment terhadap dataset yang ada.
Pandas sendiri telah memberikan tools bawaan untuk melakukan hal tersebut.

Yakni:

`dropna()` untuk menghapus nilai yang kosong

`drop_duplicates()` untuk menghapus nilai yang duplikat
"""

df = df.dropna()
#Potonga ini berguna untuk menghapus data yang memiliki kolom kosong dalam baris tersebut

df = df.drop_duplicates()
#Potongan Ini berguna untuk menghapus data yang duplikat

"""# Preparasi Untuk Content-Based Collaboration

## Embedding
Embed setiap genre dengan tfid.

Ini dilakukan untuk mengubah untuk mengubah setiap huruf menjadi angka dengan nilai kepentingannya yang disesuaikan pada corpus yang kita miliki.
"""

tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(df['genres'])
tfidf_vectorizer.get_feature_names_out()

#Melakukan embed, kemudian dilakukan pengecekan, fitur apa saja yang didapat
#melalui proses embedding tersebut.

"""## Calculate Distance
Pengukuran kesamaan menggunakan cosinus
"""

n_neighbors = 6
nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm='brute')
nn_model.fit(tfidf_matrix)

#Dikarenakan colab memiliki memory yang terbatas, dan menggunakan pearson corelation dan
#Cosine Similarity Function akan membuat muat ram yang berlebihan, maka disini kita
#akan emnggunakan nearest neighbors dengan algoritma brute force dan pendekatan cosine
#distance untuk mencari nilai jarak kosinus

distances, indices = nn_model.kneighbors(tfidf_matrix)
similarities = 1 - distances

#Menghitung kemiripan dengan menggunakan hasil dari fitting model

"""## Preparasi Untuk Collaborative Learning"""

user_ids = df["userId"].unique().tolist()
movie_ids = df["movieId"].unique().tolist()
#Mengambil id user dan juga film yang memang unik, untuk memastikan tidak adanya
#nilai yang terduplikasi.
#Nilai id yang telah diambil tadi di ubah menjadi List.

"""Melakukan encode agar lebih mudah dimasukkan ke model

Ini akan menghasilkan nilai dari yang seperti:

[123,321,223]

menjadi:

[1,2,3]
"""

user_idx = {x: i for i, x in enumerate(user_ids)}
movie_idx = {x: i for i, x in enumerate(movie_ids)}
#Melakukan mapping untuk di aplikasikan terhadap indexing dataset

movie_decoded = {i : x for i, x in enumerate(movie_ids)}  #mengubah kembali, untuk inferencing

df["userIdx"] = df["userId"].map(user_idx)
df["movieIdx"] = df["movieId"].map(movie_idx)
#Mengaplikasi map yang telah di buat.

df["rating"] = df["rating"].values.astype(np.float32) # konversi menjadi float

#minmax scaling
min_rating = min(df["rating"])
max_rating = max(df["rating"])

# Mengambil jumlah user dan film
num_users = len(user_idx)
num_movies = len(movie_idx)

print('Lowest Rating | Max Rating: ', end = "")
print(min_rating,"|",max_rating)
print("Num Users | Num Movies: ", end = "")
print(num_users, "|",num_movies)

#Shuffle/Kocok dataset
df = df.sample(frac=1, random_state=42)
df

"""## Membagi dataset

Disini, akan dilakukan pembagian dataset dengan scaling menggunakan minmax scaler, kemudian dataset akan di pecah dengan skala

90%10


"""

x = df[["userIdx", "movieIdx"]].values
y = df["rating"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
#Splice dataste
split = int(0.9 * df.shape[0])
x_train, x_val, x_test, y_train, y_val, y_test = (x[:split], x[split:], x[:split], y[:split], y[split:], y[split:])

"""# Training

# Content Based / System
"""

def recommend(movie_index, df, indices, similarities):
    print(f"Original Movie: {df.iloc[movie_index]['title']} {df.iloc[movie_index]['genres']}\n")
    print("Top Recommendations:")
    amt_recommend = len(indices[movie_index])
    sims = similarities[movie_index]
    rec_df = pd.DataFrame(columns=['idx', 'movie_id', 'title', 'genres', 'rating', 'similarity Score'])
    for i in range(1, amt_recommend): #Loop hingga jumlah yang di klasifikasikan sesuai pada jumlah film yang di dapatkan di similarity index similarity test
        idx = indices[movie_index][i]
        movie_id = df.iloc[idx]['movieId']
        title = df.iloc[idx]['title']
        rating = df.iloc[idx]['rating']
        genre = df.iloc[idx]['genres']
        similarity_score = similarities[movie_index][i]
        rec_df.loc[i] = [idx, movie_id, title, genre, rating, similarity_score]

    return rec_df

def calculate_precision(movie_index, df, indices, top_n_recommendations=10):
    # Ambil genre
    target_genres = set(df.loc[movie_index, 'genres'].split('|'))

    # Ambil nilai neighbors
    neighbor_indices = indices[movie_index][1:top_n_recommendations + 1]

    # Hmenghitung berapa banyak rekomendasi film yang memiliki kemiripan genre dengan film target.
    relevant_count = 0
    for i in neighbor_indices:
        candidate_genres = set(df.loc[i, 'genres'].split('|'))
        if target_genres & candidate_genres:
            relevant_count += 1

    precision = relevant_count / top_n_recommendations
    return precision

"""## Melakukan Percobaan atau Inferencing
Di tahapan ini, system yang di bangun akan di test.
"""

x = recommend(123, df, indices, similarities)
x

precision = calculate_precision(
    movie_index=123,
    df=df,
    indices=indices,
    top_n_recommendations=5
)

print(f"Genre-based Precision@5 untuk index 123: {precision:.2f}")

"""Dalam percobaan ini, kita mendapat 4 film yang direkomendasikan.


Bila diperhatikan, seluruh film ini tidak memiliki hubungan langsung dengan film Untouchables, ini merupakan nilai yang diekspektasikan karena ketika menggunakan tfidf, yang diperhatikan hanyalaah hubungan 1 kalimat ke kalimat yang lain.

# Collaborative Filtering

Membangun custom model menggunakan keras.
"""

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_users = num_users #berapa banyak user?
        self.num_movies = num_movies #berapa banyak film?
        self.embedding_size = embedding_size #Seberapa besar embedding akan dilakukan?
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        ) #melakukan embedding terhadap user menjadi nilai vector
        self.user_bias = layers.Embedding(num_users, 1) #menambah bias untuk nilai user
        self.movie_embedding = layers.Embedding(
            num_movies,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        ) #melakukan embedding terhadap film menjadi nilai vektor
        self.movie_bias = layers.Embedding(num_movies, 1) #menentukan bias untuk film

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])
        # Mencari dot products melalui perkalian matriks
        dot_user_movie = ops.tensordot(user_vector, movie_vector, 2)
        # kemudian menambah bias
        x = dot_user_movie + user_bias + movie_bias
        # Hitung nilai aktivasinya
        return ops.nn.sigmoid(x)


model = RecommenderNet(num_users, num_movies, 50)
model.compile(
    loss=keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=15,
    verbose=1,
    validation_data=(x_val, y_val),
)

# Melakukan fitting
# dengan 15 epochs dan 32 batch size

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""## Hasil
Melihat pada hasil, model tidak mengalami overfit, dan memiliki hasil yang cukup baik

# **Inferencing** || **Collaborative Learning**
"""

df.columns

# Mengambil Data User
user_id = 524

#mengambil film yang telah di nonton oleh user
movies_watched_by_user = df[df.userId == user_id]

#mengambil film yang belum ditonton oleh user
movies_not_watched = df_movies[~df_movies["movieId"].isin(movies_watched_by_user.movieId.values)]["movieId"]

#Membuatnya menjadi list unique
movies_not_watched = list(set(movies_not_watched).intersection(set(movie_idx.keys())))

# Mengambil index movie
movies_not_watched = [[movie_idx.get(x)] for x in movies_not_watched]
#mengambil index user
user_encoder = user_idx.get(user_id)

#menggabungkan index user dan film yang belum ditontonnya dan ditumpuk secara horizontal
user_movie_array = np.hstack(([[user_encoder]] * len(movies_not_watched), movies_not_watched))

print(f"id: {user_id}")
movies_watched_by_user[movies_watched_by_user['rating'] >= 4].sample(5)

"""Bila diperhatikan, film yang di sukai oleh pengguna 524 adalah yang bergenre drama, dan thriller/mysteries, dengan mayoritasnya adalah Drama."""

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1] #mengambil rekomendasi 10 film terbaik
recommended_movie_ids = [
    movie_decoded.get(movies_not_watched[x][0]) for x in top_ratings_indices
]

print("User Sample: {}".format(user_id))
print("====" * 9)
print("Film dengan penilaian tinggi dri user")
print("----" * 8)
top_movies_user = (
    movies_watched_by_user.sort_values(by="rating", ascending=False)
    .head(5)
    .movieId.values
) #mengambil 5 film dengan rating terbaik yang diberikan user
movie_df_rows = df_movies[df_movies["movieId"].isin(top_movies_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ":", row.genres)

print("----" * 8)
print("10 rekomendasi film yang akan di ambil")
print("----" * 8)
recommended_movies = df_movies[df_movies["movieId"].isin(recommended_movie_ids)]
for row in recommended_movies.itertuples():
    print(row.title, ":", row.genres)

"""## Kesimpulan Hasil
Bila diperhatikan, mayoritas daripada rekomendasinya adalah film Drama.

Ini sesuai dengan hipotesis awal, dimana akan sering ditemukan rekomendasi dengan genre Drama.
"""