# -*- coding: utf-8 -*-
"""Rev.2 Submission_Akhir_Dicoding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Cks5_eoILKDTgBrMxJIqrAyvNDfP1v8z

#AKUSISI DATASET
"""

!pip install kaggle

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download "nicoletacilibiu/movies-and-ratings-for-recommendation-system"

!unzip movies-and-ratings-for-recommendation-system.zip

"""# DATA LOADING"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import numpy as np

from tensorflow.keras import layers, ops
from tensorflow import keras
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import NearestNeighbors

df_movies = pd.read_csv("movies.csv")
df_ratings = pd.read_csv("ratings.csv")

"""# EDA"""

df_movies.info()

df_ratings.info()

df_ratings['rating'].describe()

# Menggabungkan data rating dan film menjadi satu.
df = df_movies.merge(df_ratings, on = 'movieId', how = "left")
df

df.isna().sum()

rating_group = df.groupby(['title', 'rating']).size().reset_index(name='count')
rating_group = rating_group.sort_values(by='count', ascending=False)
rating_group.head(10)

rating_group = df.groupby('title')['rating'].mean().sort_values(ascending=True).reset_index()

"""### Visualisasi"""

plt.figure(figsize=(10,6))
sns.histplot(rating_group['rating'], bins=5, kde=True)
plt.title('Distribusi Rata Rata Rating per Title')
plt.xlabel('Average Rating')
plt.show()

df_genres = df['genres'].str.split('|').explode()

genre_counts = df_genres.value_counts()

plt.figure(figsize=(12,8))
sns.barplot(x=genre_counts.values, y=genre_counts.index)
plt.title('Distribusi Genre')
plt.xlabel('Count')
plt.ylabel('Genre')
plt.show()

# Ekstrak tahun
df['year'] = df['title'].str.extract(r'\((\d{4})\)').astype(float)
plt.figure(figsize=(12,8))

# Hilangkan tahun yang hilang
years = df['year'].dropna()

# Plot histogram dengan KDE
sns.histplot(years, bins=10, kde=True, fill = False)

plt.title('Film per Tahun')
plt.xlabel('Year')
plt.ylabel('Density')
plt.show()

"""# Preprocessing

## Missing Value dan Duplicated
"""

df.isna().sum()

df = df.dropna()

print(df.duplicated().sum())

"""# Preparasi Untuk Content-Based Collaboration

## Embedding
Embed setiap genre dengan tfid
"""

tfidf_vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(df['genres'])
tfidf_vectorizer.get_feature_names_out()

"""## Calculate Distance
Pengukuran kesamaan menggunakan cosinus
"""

n_neighbors = 6
nn_model = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm='brute')
nn_model.fit(tfidf_matrix)

distances, indices = nn_model.kneighbors(tfidf_matrix)
similarities = 1 - distances

"""## Preparasi Untuk Collaborative Learning"""

user_ids = df["userId"].unique().tolist()
movie_ids = df["movieId"].unique().tolist()

"""Melakukan encode agar lebih mudah dimasukkan ke model"""

user_idx = {x: i for i, x in enumerate(user_ids)}
movie_idx = {x: i for i, x in enumerate(movie_ids)}


movie_decoded = {i : x for i, x in enumerate(movie_ids)}  #mengubah kembali, untuk inferencing

df["userIdx"] = df["userId"].map(user_idx)
df["movieIdx"] = df["movieId"].map(movie_idx)

df["rating"] = df["rating"].values.astype(np.float32) # konversi menjadi float

#minmax scaling
min_rating = min(df["rating"])
max_rating = max(df["rating"])

# Mengambil jumlah user dan film
num_users = len(user_idx)
num_movies = len(movie_idx)

print('Lowest Rating | Max Rating: ', end = "")
print(min_rating,"|",max_rating)
print("Num Users | Num Movies: ", end = "")
print(num_users, "|",num_movies)

#Shuffle/Kocok dataset
df = df.sample(frac=1, random_state=42)
df

x = df[["userIdx", "movieIdx"]].values
y = df["rating"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

#Splice dataste
split = int(0.9 * df.shape[0])
x_train, x_val, x_test, y_train, y_val, y_test = (x[:split], x[split:], x[:split], y[:split], y[split:], y[split:])

"""# Training

# Content Based / System
"""

def recommend(movie_index, df, indices, similarities):
    print(f"Original Movie: {df.iloc[movie_index]['title']} {df.iloc[movie_index]['genres']}\n")
    print("Top Recommendations:")
    amt_recommend = len(indices[movie_index])
    sims = similarities[movie_index]
    rec_df = pd.DataFrame(columns=['idx', 'movie_id', 'title', 'genres', 'rating', 'similarity Score'])
    for i in range(1, amt_recommend): #Loop hingga jumlah yang di klasifikasikan sesuai pada jumlah film yang di dapatkan di similarity index similarity test
        idx = indices[movie_index][i]
        movie_id = df.iloc[idx]['movieId']
        title = df.iloc[idx]['title']
        rating = df.iloc[idx]['rating']
        genre = df.iloc[idx]['genres']
        similarity_score = similarities[movie_index][i]
        rec_df.loc[i] = [idx, movie_id, title, genre, rating, similarity_score]

    return rec_df

def calculate_precision(movie_index, df, indices, top_n_recommendations=10):
    # Ambil genre
    target_genres = set(df.loc[movie_index, 'genres'].split('|'))

    # Ambil nilai neighbors
    neighbor_indices = indices[movie_index][1:top_n_recommendations + 1]

    # Hmenghitung berapa banyak rekomendasi film yang memiliki kemiripan genre dengan film target.
    relevant_count = 0
    for i in neighbor_indices:
        candidate_genres = set(df.loc[i, 'genres'].split('|'))
        if target_genres & candidate_genres:
            relevant_count += 1

    precision = relevant_count / top_n_recommendations
    return precision

"""## Melakukan Percobaan atau Inferencing
Di tahapan ini, system yang di bangun akan di test.
"""

x = recommend(123, df, indices, similarities)
x

precision = calculate_precision(
    movie_index=123,
    df=df,
    indices=indices,
    top_n_recommendations=5
)

print(f"Genre-based Precision@5 untuk index 123: {precision:.2f}")

"""Dalam percoaan ini, kita mendapat 4 film yang direkomendasikan.


Bila diperhatikan, seluruh film ini tidak memiliki hubungan langsung dengan film Untouchables, ini merupakan nilai yang diekspektasikan karena ketika menggunakan tfidf, yang diperhatikan hanyalaah hubungan 1 kalimat ke kalimat yang lain.

# Collaborative Filtering

Membangun custom model menggunakan keras.
"""

class RecommenderNet(keras.Model):
    def __init__(self, num_users, num_movies, embedding_size, **kwargs):
        super().__init__(**kwargs)
        self.num_users = num_users #berapa banyak user?
        self.num_movies = num_movies #berapa banyak film?
        self.embedding_size = embedding_size #Seberapa besar embedding akan dilakukan?
        self.user_embedding = layers.Embedding(
            num_users,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        ) #melakukan embedding terhadap user menjadi nilai vector
        self.user_bias = layers.Embedding(num_users, 1) #menambah bias untuk nilai user
        self.movie_embedding = layers.Embedding(
            num_movies,
            embedding_size,
            embeddings_initializer="he_normal",
            embeddings_regularizer=keras.regularizers.l2(1e-6),
        ) #melakukan embedding terhadap film menjadi nilai vektor
        self.movie_bias = layers.Embedding(num_movies, 1) #menentukan bias untuk film

    def call(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])
        # Mencari dot products melalui perkalian matriks
        dot_user_movie = ops.tensordot(user_vector, movie_vector, 2)
        # kemudian menambah bias
        x = dot_user_movie + user_bias + movie_bias
        # Hitung nilai aktivasinya
        return ops.nn.sigmoid(x)


model = RecommenderNet(num_users, num_movies, 50)
model.compile(
    loss=keras.losses.BinaryCrossentropy(),
    optimizer=keras.optimizers.Adam(learning_rate=1e-3),
)

history = model.fit(
    x=x_train,
    y=y_train,
    batch_size=32,
    epochs=15,
    verbose=1,
    validation_data=(x_val, y_val),
)

plt.plot(history.history["loss"])
plt.plot(history.history["val_loss"])
plt.title("model loss")
plt.ylabel("loss")
plt.xlabel("epoch")
plt.legend(["train", "test"], loc="upper left")
plt.show()

"""# **Inferencing**"""

# Mengambil Data User secara acak
user_id = df.userId.sample(1).iloc[0]
movies_watched_by_user = df[df.userId == user_id]
movies_not_watched = df_movies[~df_movies["movieId"].isin(movies_watched_by_user.movieId.values)]["movieId"]
movies_not_watched = list(set(movies_not_watched).intersection(set(movie_idx.keys())))
movies_not_watched = [[movie_idx.get(x)] for x in movies_not_watched]
user_encoder = user_idx.get(user_id)
user_movie_array = np.hstack(([[user_encoder]] * len(movies_not_watched), movies_not_watched))

print(f"id: {user_id}")
movies_watched_by_user[movies_watched_by_user['rating'] >= 4].sample(5)

"""Bila diperhaitkan, film yang di sukai oleh pengguna 599 adalah yang bersifat drama, dan thriller/mysteries, dengan mayoritasnya adalah Drama."""

ratings = model.predict(user_movie_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1] #mengambil rekomendasi 10 film terbaik
recommended_movie_ids = [
    movie_decoded.get(movies_not_watched[x][0]) for x in top_ratings_indices
]

print("User Sample: {}".format(user_id))
print("====" * 9)
print("Film dengan penilaian tinggi dri user")
print("----" * 8)
top_movies_user = (
    movies_watched_by_user.sort_values(by="rating", ascending=False)
    .head(5)
    .movieId.values
) #mengambil 5 film dengan rating terbaik yang diberikan user
movie_df_rows = df_movies[df_movies["movieId"].isin(top_movies_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ":", row.genres)

print("----" * 8)
print("10 rekomendasi film yang akan di ambil")
print("----" * 8)
recommended_movies = df_movies[df_movies["movieId"].isin(recommended_movie_ids)]
for row in recommended_movies.itertuples():
    print(row.title, ":", row.genres)

"""Kita mendapat film yang mayoritasnya drama melalui sistem rekomendasi yang telah dibaut"""